---
title: "Untitled"
output: html_document
date: "2022-12-11"
bibliography: citations.bib

---

```{r, include = FALSE}
library(knitr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(tm)
library(SnowballC)
library(syuzhet)
library(gtrendsR)
remotes::install_github("news-r/nytimes")
library(jsonlite)
library(stringr)
library(tidytext)
library(textdata)
library(ggpubr)
```

## Introduction

  This project examines headlines and snippets regarding unemployment, specifically, headlines from the New York Times. Starting from 2008, the middle of a recession, to present day, the headlines and snippets will be analyzed to see the commonly frequented words and how they have changed throughout the years. The keywords in the headers may also highlight other concerns that Americans have that is not related to healthcare. Therefore, this project will answer the following questions :
  1. How has the wording in headlines and snippets changed regarding unemployment from 2008 to 2022?
  2. Can headline and snippet wording regarding unemployment shed light into other concerns that Americans may have?

### Data Collection

The New York Times API will be used to collect headlines for the specific time range regarding unemployment. This sample of code. An RPubs tutorial released aided in collecting the data as the author, Vicentwx, went through the article search function and the text cleaning [@vincentwx_2019].




```{r include = FALSE}
nytime = function (keyword,year) {
  searchQ = URLencode(keyword)
  url = paste('http://api.nytimes.com/svc/search/v2/articlesearch.json?q=',searchQ,
              '&begin_date=',year,'0101&end_date=',year,'1231&api-key=',api,sep="")
 
  initialsearch = fromJSON(url,flatten = T)
  maxPages = round((initialsearch$response$meta$hits / 10)-1)
  
 
  maxPages = ifelse(maxPages >= 10, 10, maxPages)
  
 
  df = data.frame(id=as.numeric(),created_time=character(),snippet=character(),
                  headline=character())
  
 
  for(i in 0:maxPages){
  
    nytSearch = fromJSON(paste0(url, "&page=", i), flatten = T) 
    temp = data.frame(id=1:nrow(nytSearch$response$docs),
                      created_time = nytSearch$response$docs$pub_date,
                      snippet = nytSearch$response$docs$snippet,
                      headline = nytSearch$response$docs$headline.main)
    df=rbind(df,temp)
    Sys.sleep(5) 
  }
  return(df)
}
```


```{r, include = FALSE}

# These are all in seperate code chunks that the API does not get overwhelmed.

api = "ZMG66AOpWqDJ5OqyK9GSZh4mSrWAyapR"
NY2008 <- nytime('unemployment',2008)
```

```{r, include = FALSE}
NY2010 <- nytime('unemployment',2010)
```

```{r, include = FALSE}
NY2012 <- nytime('unemployment',2012)
```

```{r, include = FALSE}
NY2014 <- nytime('unemployment',2014)
```

```{r, include = FALSE}
NY2016 <- nytime('unemployment',2016)
```

```{r, include = FALSE}
NY2018 <- nytime('unemployment',2018)
```

```{r, include = FALSE}
NY2020 <- nytime('unemployment',2020)
```

```{r, include = FALSE}
NY2022 <- nytime('unemployment',2022)
```

```{r include = FALSE}
write.csv(NY2022, "C:\\Users\\tutu\\Downloads\\NY2022.csv")
write.csv(NY2020, "C:\\Users\\tutu\\Downloads\\NY2020.csv")
write.csv(NY2018, "C:\\Users\\tutu\\Downloads\\NY2018.csv")
write.csv(NY2016, "C:\\Users\\tutu\\Downloads\\NY2016.csv")
write.csv(NY2014, "C:\\Users\\tutu\\Downloads\\NY2014.csv")
write.csv(NY2012, "C:\\Users\\tutu\\Downloads\\NY2012.csv")
write.csv(NY2010, "C:\\Users\\tutu\\Downloads\\NY20110.csv")
write.csv(NY2008, "C:\\Users\\tutu\\Downloads\\NY2008.csv")
```


### Cleaning and Preparing the Data
For the cleaning and preparation, the data cleaning process by Vicentwx [@vincentwx_2019] was a very detailed guide in removing all unnecessary punctuation, words and white spaces. An example of one of the punctuation removal is below:

```{r, include = FALSE}
# Adding the appropriate year to the data
U2008<-data.frame(append(NY2008, c(Year='2008'), after=1))

U2010<-data.frame(append(NY2010, c(Year='2010'), after=1))

U2012<-data.frame(append(NY2012, c(Year='2012'), after=1))

U2014<-data.frame(append(NY2014, c(Year='2014'), after=1))

U2016<-data.frame(append(NY2016, c(Year='2016'), after=1))

U2018<-data.frame(append(NY2018, c(Year='2018'), after=1))

U2020<-data.frame(append(NY2020,c(Year='2020'), after=1))

U2022<-data.frame(append(NY2022, c(Year='2022'), after=1))
```


```{r, include = FALSE}
TextDoc2008 <- Corpus(VectorSource(U2008))

edit_text <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

TextDoc2008 <- tm_map(TextDoc2008, edit_text, "/")
TextDoc2008 <- tm_map(TextDoc2008, edit_text, "@")
TextDoc2008 <- tm_map(TextDoc2008, edit_text, "\\|")
TextDoc2008 <- tm_map(TextDoc2008, content_transformer(tolower))
TextDoc2008 <- tm_map(TextDoc2008, removeNumbers)
TextDoc2008 <- tm_map(TextDoc2008, removeWords, stopwords("english"))
TextDoc2008 <- tm_map(TextDoc2008, removePunctuation)
TextDoc2008 <- tm_map(TextDoc2008, stripWhitespace)
TextDoc2010 <- tm_map(TextDoc2008, stemDocument)

head(TextDoc2008)
TextDoc_dtm_2008 <- TermDocumentMatrix(TextDoc2008)

TextDoc_dtm_2008 <- TermDocumentMatrix(TextDoc2008)
dtm_m_2008 <- as.matrix(TextDoc_dtm_2008)
```


```{r, include = FALSE}
dtm_v_2008 <- sort(rowSums(dtm_m_2008),decreasing=TRUE)
dtm_d_2008 <- data.frame(word = names(dtm_v_2008),freq=dtm_v_2008)

```

Below is a table that shows the top 5 words for articles regarding unemployment in the headlines and snippets. Later on, I have decided to not include the word "unemployment" and "jobs" in some of the data analysis to ensure that other words are examined more. 

```{r}
library(gt)
head(dtm_d_2008, 5)

dtm_d_2008%>%
  head(5)%>%
  gt()%>%
  tab_header(title="2008 Top Words")
```

```{r, include = FALSE}
TextDoc2010 <- Corpus(VectorSource(U2010))

edit_text <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

TextDoc2010 <- tm_map(TextDoc2010, edit_text, "/")
TextDoc2010 <- tm_map(TextDoc2010, edit_text, "@")
TextDoc2010 <- tm_map(TextDoc2010, edit_text, "\\|")
TextDoc2010 <- tm_map(TextDoc2010, content_transformer(tolower))
TextDoc2010 <- tm_map(TextDoc2010, removeNumbers)
TextDoc2010 <- tm_map(TextDoc2010, removeWords, stopwords("english"))
TextDoc2010 <- tm_map(TextDoc2010, removePunctuation)
TextDoc2010 <- tm_map(TextDoc2010, stripWhitespace)
TextDoc2010 <- tm_map(TextDoc2010, stemDocument)

head(TextDoc2010)
TextDoc_dtm_2010 <- TermDocumentMatrix(TextDoc2010)

TextDoc_dtm_2010 <- TermDocumentMatrix(TextDoc2010)
dtm_m_2010 <- as.matrix(TextDoc_dtm_2010)

# Sort by descearing value of frequency
dtm_v_2010 <- sort(rowSums(dtm_m_2010),decreasing=TRUE)
dtm_d_2010 <- data.frame(word = names(dtm_v_2010),freq=dtm_v_2010)
# Display the top 5 most frequent words
head(dtm_d_2010, 5)
```

```{r, include = FALSE}
TextDoc2012 <- Corpus(VectorSource(U2012))

edit_text <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

TextDoc2012 <- tm_map(TextDoc2012, edit_text, "/")
TextDoc2012 <- tm_map(TextDoc2012, edit_text, "@")
TextDoc2012 <- tm_map(TextDoc2012, edit_text, "\\|")
TextDoc2012 <- tm_map(TextDoc2012, content_transformer(tolower))
TextDoc2012 <- tm_map(TextDoc2012, removeNumbers)
TextDoc2012 <- tm_map(TextDoc2012, removeWords, stopwords("english"))
TextDoc2012 <- tm_map(TextDoc2012, removePunctuation)
TextDoc2012 <- tm_map(TextDoc2012, stripWhitespace)
TextDoc2012 <- tm_map(TextDoc2012, stemDocument)

head(TextDoc2012)
TextDoc_dtm_2012 <- TermDocumentMatrix(TextDoc2012)

TextDoc_dtm_2012 <- TermDocumentMatrix(TextDoc2012)
dtm_m_2012 <- as.matrix(TextDoc_dtm_2012)

# Sort by descearing value of frequency
dtm_v_2012 <- sort(rowSums(dtm_m_2012),decreasing=TRUE)
dtm_d_2012 <- data.frame(word = names(dtm_v_2012),freq=dtm_v_2012)
# Display the top 5 most frequent words
head(dtm_d_2012, 5)
```

```{r, include = FALSE}
TextDoc2014 <- Corpus(VectorSource(U2014))

edit_text <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

TextDoc2014 <- tm_map(TextDoc2014, edit_text, "/")
TextDoc2014 <- tm_map(TextDoc2014, edit_text, "@")
TextDoc2014 <- tm_map(TextDoc2014, edit_text, "\\|")
TextDoc2014 <- tm_map(TextDoc2014, content_transformer(tolower))
TextDoc2014 <- tm_map(TextDoc2014, removeNumbers)
TextDoc2014 <- tm_map(TextDoc2014, removeWords, stopwords("english"))
TextDoc2014 <- tm_map(TextDoc2014, removePunctuation)
TextDoc2014 <- tm_map(TextDoc2014, stripWhitespace)
TextDoc2014 <- tm_map(TextDoc2014, stemDocument)

head(TextDoc2014)
TextDoc_dtm_2014 <- TermDocumentMatrix(TextDoc2014)

TextDoc_dtm_2014 <- TermDocumentMatrix(TextDoc2014)
dtm_m_2014 <- as.matrix(TextDoc_dtm_2014)

# Sort by descearing value of frequency
dtm_v_2014 <- sort(rowSums(dtm_m_2014),decreasing=TRUE)
dtm_d_2014 <- data.frame(word = names(dtm_v_2014),freq=dtm_v_2014)
# Display the top 5 most frequent words
head(dtm_d_2014, 5)
```

```{r, include = FALSE}
TextDoc2016 <- Corpus(VectorSource(U2016))

edit_text <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

TextDoc2016 <- tm_map(TextDoc2016, edit_text, "/")
TextDoc2016 <- tm_map(TextDoc2016, edit_text, "@")
TextDoc2016 <- tm_map(TextDoc2016, edit_text, "\\|")
TextDoc2016 <- tm_map(TextDoc2016, content_transformer(tolower))
TextDoc2016 <- tm_map(TextDoc2016, removeNumbers)
TextDoc2016 <- tm_map(TextDoc2016, removeWords, stopwords("english"))
TextDoc2016 <- tm_map(TextDoc2016, removePunctuation)
TextDoc2016 <- tm_map(TextDoc2016, stripWhitespace)
TextDoc2016 <- tm_map(TextDoc2016, stemDocument)

head(TextDoc2016)
TextDoc_dtm_2016 <- TermDocumentMatrix(TextDoc2016)

TextDoc_dtm_2016 <- TermDocumentMatrix(TextDoc2016)
dtm_m_2016 <- as.matrix(TextDoc_dtm_2016)

# Sort by descearing value of frequency
dtm_v_2016 <- sort(rowSums(dtm_m_2016),decreasing=TRUE)
dtm_d_2016 <- data.frame(word = names(dtm_v_2016),freq=dtm_v_2016)
# Display the top 5 most frequent words
head(dtm_d_2016, 5)
```


```{r, include = FALSE}
TextDoc2018 <- Corpus(VectorSource(U2018))

edit_text <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

TextDoc2018 <- tm_map(TextDoc2018, edit_text, "/")
TextDoc2018 <- tm_map(TextDoc2018, edit_text, "@")
TextDoc2018 <- tm_map(TextDoc2018, edit_text, "\\|")
TextDoc2018 <- tm_map(TextDoc2018, content_transformer(tolower))
TextDoc2018 <- tm_map(TextDoc2018, removeNumbers)
TextDoc2018 <- tm_map(TextDoc2018, removeWords, stopwords("english"))
TextDoc2018 <- tm_map(TextDoc2018, removePunctuation)
TextDoc2018 <- tm_map(TextDoc2018, stripWhitespace)
TextDoc2018 <- tm_map(TextDoc2018, stemDocument)

head(TextDoc2018)
TextDoc_dtm_2018 <- TermDocumentMatrix(TextDoc2018)

TextDoc_dtm_2018 <- TermDocumentMatrix(TextDoc2018)
dtm_m_2018 <- as.matrix(TextDoc_dtm_2018)

# Sort by descearing value of frequency
dtm_v_2018 <- sort(rowSums(dtm_m_2018),decreasing=TRUE)
dtm_d_2018 <- data.frame(word = names(dtm_v_2018),freq=dtm_v_2018)
# Display the top 5 most frequent words
head(dtm_d_2018, 5)
```

```{r, include = FALSE}
TextDoc2020 <- Corpus(VectorSource(U2020))

edit_text <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

TextDoc2020 <- tm_map(TextDoc2020, edit_text, "/")
TextDoc2020 <- tm_map(TextDoc2020, edit_text, "@")
TextDoc2020 <- tm_map(TextDoc2020, edit_text, "\\|")
TextDoc2020 <- tm_map(TextDoc2020, content_transformer(tolower))
TextDoc2020 <- tm_map(TextDoc2020, removeNumbers)
TextDoc2020 <- tm_map(TextDoc2020, removeWords, stopwords("english"))
TextDoc2020 <- tm_map(TextDoc2020, removePunctuation)
TextDoc2020 <- tm_map(TextDoc2020, stripWhitespace)
TextDoc2020 <- tm_map(TextDoc2020, stemDocument)

head(TextDoc2020)
TextDoc_dtm_2020 <- TermDocumentMatrix(TextDoc2020)

TextDoc_dtm_2020 <- TermDocumentMatrix(TextDoc2020)
dtm_m_2020 <- as.matrix(TextDoc_dtm_2020)

# Sort by descearing value of frequency
dtm_v_2020 <- sort(rowSums(dtm_m_2020),decreasing=TRUE)
dtm_d_2020 <- data.frame(word = names(dtm_v_2020),freq=dtm_v_2020)
# Display the top 5 most frequent words
head(dtm_d_2020, 5)
```

```{r, include = FALSE}
TextDoc2022 <- Corpus(VectorSource(U2022))

edit_text <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

TextDoc2022 <- tm_map(TextDoc2022, edit_text, "/")
TextDoc2022 <- tm_map(TextDoc2022, edit_text, "@")
TextDoc2022 <- tm_map(TextDoc2022, edit_text, "\\|")
TextDoc2022 <- tm_map(TextDoc2022, content_transformer(tolower))
TextDoc2022 <- tm_map(TextDoc2022, removeNumbers)
TextDoc2022 <- tm_map(TextDoc2022, removeWords, stopwords("english"))
TextDoc2022 <- tm_map(TextDoc2022, removePunctuation)
TextDoc2022 <- tm_map(TextDoc2022, stripWhitespace)
TextDoc2022 <- tm_map(TextDoc2022, stemDocument)

head(TextDoc2022)
TextDoc_dtm_2022 <- TermDocumentMatrix(TextDoc2022)

TextDoc_dtm_2022 <- TermDocumentMatrix(TextDoc2022)
dtm_m_2022 <- as.matrix(TextDoc_dtm_2022)

# Sort by descearing value of frequency
dtm_v_2022 <- sort(rowSums(dtm_m_2022),decreasing=TRUE)
dtm_d_2022 <- data.frame(word = names(dtm_v_2022),freq=dtm_v_2022)
# Display the top 5 most frequent words
plot(dtm_d_2022$word, dtm_m_2018$freq)
head(dtm_d_2022)
dtm_d_2022
barplot(dtm_d_2022[1:5,]$freq, las = 2, names.arg = dtm_d_2022[1:5,]$word,
        col ="lightgreen", main ="Top 5 most frequent words",
        ylab = "Word frequencies")
```

### Explanatory Analysis
After the data cleaning and preparing is complete, bar charts are made to look at the data. I have removed common words such as "employment" as mentioned before. It is common for articles to include those words since it is a a keyword that was used to search for the articles. Therefore, we want to examine other words that may give insight on the employment issues, concerns, and general ideas each specific year. 

```{r, include = FALSE}
head(dtm_d_2008)
dtm_d_2008$word.f <-as.factor(dtm_d_2008$word)
top2008<-dtm_d_2008%>%
  slice(3:8)

plot_08<-ggplot(data= top2008,aes(word.f, freq,fill=freq)) +    
  geom_bar(stat="identity",   
           position=position_dodge(),) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Top Frequented Words in NYT Articles \n Regarding Unemployment in 2008")+
  xlab("Word")+ ylab("Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 45)
```

### 2008

The top words in the 2008 headlines and snippets are economic, economy, jobless, jobs, new, rate, and york.

From this chart, I could see one of the limitations to this project. Similar words such as economy and economic should have been combined together so that their counts could have been added together. Another limitation is the inability to tell if the word "new" and "york" has articles focusing on the city New York or if the articles are mentioning the New York Times. In a future analysis, the word "New York Times" may be omitted. 

```{r}
plot_08
```

```{r, include = FALSE}
head(dtm_d_2010)
dtm_d_2010$word.f <-as.factor(dtm_d_2010$word)
top2010<-dtm_d_2010%>%
  slice(3:8)

plot_10<-ggplot(data= top2010,aes(word.f, freq,fill=freq)) +    
  geom_bar(stat="identity",   
           position=position_dodge(),) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Top Frequented Words in NYT Articles \n Regarding Unemployment in 2010")+
  xlab("Word")+ ylab("Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 45)

plot_10
```
### 2010
The top frequented words for NYT articles in 2010 is economi, jobless, new, Obama, rate, and recess. The word "Obama" was the most interesting to see as the word was not a top frequenter in 2008. Most likely due to Obama being elected into office in 2008. The jobless, rate, new and economi are still top frequenters.
```{r}
plot_10
```

```{r, include=FALSE}
head(dtm_d_2012)
dtm_d_2012$word.f <-as.factor(dtm_d_2012$word)
top2012<-dtm_d_2012%>%
  slice(3:8)

plot_12<-ggplot(data= top2012,aes(word.f, freq,fill=freq)) +    
  geom_bar(stat="identity",   
           position=position_dodge(),) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Top Frequented Words in NYT Articles \n Regarding Unemployment in 2012")+
  xlab("Word")+ ylab("Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 45)
```

### 2012

Not much is different between 2012 and the previous years. There are less articles using the word "jobless" than before. 
```{r}
plot_12
```


```{r, include = FALSE}
head(dtm_d_2014)
dtm_d_2014$word.f <-as.factor(dtm_d_2014$word)
top2014<-dtm_d_2014%>%
  slice(3:8)

plot_14<-ggplot(data= top2014,aes(word.f, freq,fill=freq)) +    
  geom_bar(stat="identity",   
           position=position_dodge(),) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Top Frequented Words in NYT Articles \n Regarding Unemployment in 2014")+
  xlab("Word")+ ylab("Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 45)
```

### 2014
In 2014, the top word in articles regarding unemployment is the word "rate" and is higher in frequency than in past years.
```{r}
plot_14
```

### 2016
```{r, include = FALSE}
head(dtm_d_2016)
dtm_d_2016$word.f <-as.factor(dtm_d_2016$word)
top2016<-dtm_d_2016%>%
  slice(2,4:8)

plot_16<-ggplot(data= top2016,aes(word.f, freq,fill=freq)) +    
  geom_bar(stat="identity",   
           position=position_dodge(),) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Top Frequented Words in NYT Articles \n Regarding Unemployment in 2016")+
  xlab("Word")+ ylab("Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 45)
```

In 2016, there is a high frequency of articles including economy. One of the reasons for increase in articles revolving economy could be due to Former President Trump being elected into office. 
```{r}
plot_16
```

### 2018
```{r, include= FALSE}
head(dtm_d_2018)
dtm_d_2018$word.f <-as.factor(dtm_d_2018$word)
top2018<-dtm_d_2018%>%
  slice(3:8)

plot_18<-ggplot(data= top2018,aes(word.f, freq,fill=freq)) +    
  geom_bar(stat="identity",   
           position=position_dodge(),) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Top Frequented Words in NYT Articles \n Regarding Unemployment in 2018")+
  xlab("Word")+ ylab("Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 45)
```
In 2018, the articles including unemployment and economy is higher than previous years including 2016. Business is also a top word that has not appeared in the top in past years. 
```{r}
plot_18
```
### 2020
```{r, include=FALSE}
head(dtm_d_2020)
dtm_d_2020$word.f <-as.factor(dtm_d_2020$word)
top2020<-dtm_d_2020%>%
  slice(2,4:8)

plot_20<-ggplot(data= top2020,aes(word.f, freq,fill=freq)) +    
  geom_bar(stat="identity",   
           position=position_dodge(),) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Top Frequented Words in NYT Articles \n Regarding Unemployment in 2020")+
  xlab("Word")+ ylab("Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 45)
```
One of the top words for this year is pandemic as COVID-19 led to shut downs and job loss concerns. People began turning to benefits due to unemployment and shut downs that forced many to stay at home. 
```{r}
plot_20
```
### 2022

```{r, include= FALSE}
head(dtm_d_2022)
dtm_d_2022$word.f <-as.factor(dtm_d_2022$word)
top2022<-dtm_d_2022%>%
  slice(1,3:4,6:8)

plot_22<-ggplot(data= top2022,aes(word.f, freq,fill=freq)) +    
  geom_bar(stat="identity",   
           position=position_dodge(),) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Top Frequented Words in NYT Articles \n Regarding Unemployment in 2022")+
  xlab("Word")+ ylab("Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 45)
```
In 2022, economy is still an important word as well as rate. The word "inflation" is a top frequent word that has not appeared in previous years.  
```{r}
plot_22
```

Below is a plot of all the bar charts combined. 
```{r echo=FALSE ,fig.width=20, fig.height=10}
ggarrange(plot_08,plot_10,plot_12,plot_14,plot_16,plot_18,plot_20,plot_22)
```

```{r, include = FALSE}
dtm_d_2008.copy<-data.frame(append(dtm_d_2008, c(Year='2008'), after=1))

dtm_d_2010.copy<-data.frame(append(dtm_d_2010, c(Year='2010'), after=1))

dtm_d_2012.copy<-data.frame(append(dtm_d_2012, c(Year='2012'), after=1))

dtm_d_2014.copy<-data.frame(append(dtm_d_2014, c(Year='2014'), after=1))

dtm_d_2016.copy<-data.frame(append(dtm_d_2016, c(Year='2016'), after=1))

dtm_d_2018.copy<-data.frame(append(dtm_d_2018, c(Year='2018'), after=1))

dtm_d_2020.copy<-data.frame(append(dtm_d_2020,c(Year='2020'), after=1))

dtm_d_2022.copy<-data.frame(append(dtm_d_2022, c(Year='2022'), after=1))

total <-rbind(dtm_d_2008.copy,dtm_d_2010.copy,dtm_d_2012.copy,dtm_d_2014.copy,dtm_d_2016.copy,dtm_d_2018.copy,dtm_d_2020.copy,dtm_d_2022.copy)

total

a<-unique(total$word.f)
a<-as.data.frame(a)
a$freq_2008<-dtm_d_2008$freq


a <- cbind(a, dtm_d_2008[c("freq")])

head(dtm_d_2008)
```



```{r}
top08<-dtm_d_2008%>%
  slice(1:6)
top10<-dtm_d_2010%>%
  slice(1:6)
top12<-dtm_d_2012%>%
  slice(1:6)
top14<-dtm_d_2014%>%
  slice(1:6)
top16<-dtm_d_2016%>%
  slice(1:6)
top18<-dtm_d_2018%>%
  slice(1:6)
top20<-dtm_d_2020%>%
  slice(1:6)
top22<-dtm_d_2022%>%
  slice(1:6)

dtm_d_2008.copy<-data.frame(append(top08, c(Year='2008'), after=1))

dtm_d_2010.copy<-data.frame(append(top10, c(Year='2010'), after=1))

dtm_d_2012.copy<-data.frame(append(top12, c(Year='2012'), after=1))

dtm_d_2014.copy<-data.frame(append(top14, c(Year='2014'), after=1))

dtm_d_2016.copy<-data.frame(append(top16, c(Year='2016'), after=1))

dtm_d_2018.copy<-data.frame(append(top18, c(Year='2018'), after=1))

dtm_d_2020.copy<-data.frame(append(top20,c(Year='2020'), after=1))

dtm_d_2022.copy<-data.frame(append(top12, c(Year='2022'), after=1))

total1 <-rbind(dtm_d_2008.copy,dtm_d_2010.copy,dtm_d_2012.copy,dtm_d_2014.copy,dtm_d_2016.copy,dtm_d_2018.copy,dtm_d_2020.copy,dtm_d_2022.copy)

total$Year<-factor(total$Year)
group_by()
dtm_d_2022.copy
top08
top10
top12
top14
top16
top18
top20
top22
```
### Time Series Plot
It would be interesting to see how top 
```{r echo=FALSE}
word_econ<-filter(total, word.f=="economi"|word.f=="economy")
word_econ$Year<-as.numeric(word_econ$Year)
word_econ


word_benefit<-filter(total, word.f=="benefit")
word_benefit$Year<-as.numeric(word_benefit$Year)
word_benefit

word_rate<-filter(total, word.f=="rate")
word_rate$Year<-as.numeric(word_rate$Year)
word_rate

word_r<-filter(total, word.f=="unemploy"|word.f=="unemployment")
word_r$Year<-as.numeric(word_r$Year)
word_r



word_econ
Complete.ts.plot <- ggplot()+
  geom_line(data=word_econ, aes(x=Year, y=freq), color="blue")+
  geom_line(data=word_benefit, aes(x=Year, y=freq), color='green')+
  geom_line(data=word_rate, aes(x=Year, y=freq), color='purple')+
  geom_line(data=word_r, aes(x=Year, y=freq), color='black')+
  ggtitle("Frequency of NYT Words")+
  xlab("Year")+
  ylab("Freq")+
  theme(plot.title = element_text(hjust = 0.5))+ 
  theme_minimal() +
     scale_color_manual(name = "Model fit",
                        breaks = c("economy", "benefit", "rate","unemployment"),
                        values = c("economy" = "blue", "benefit" = "green", "rate" = "purple", "unemployment"="black") ) # sadly the legend did not work

Complete.ts.plot
```


##Gtrends -
```{r echo=FALSE}
econ_08.1 <- gtrends(keyword="economy", time="2007-01-01 2022-12-11",geo="US")

econ_08<-as.data.frame(econ_08.1$interest_over_time)
library(lubridate)
econ_08$year <- year(ymd(econ_08$date))
econ_08$month <- month(ymd(econ_08$date)) 
econ_08$day <- day(ymd(econ_08$date))

econ_08

unemp <- gtrends(keyword="Unemployment", time="2007-01-01 2022-12-11",geo="US")

unemp<-as.data.frame(unemp$interest_over_time)
unemp$year <- year(ymd(unemp$date))
unemp$month <- month(ymd(unemp$date)) 
unemp$day <- day(ymd(unemp$date))

unemp


infla <- gtrends(keyword="inflation", time="2007-01-01 2022-12-11",geo="US")

infla<-as.data.frame(infla$interest_over_time)
infla$year <- year(ymd(infla$date))
infla$month <- month(ymd(infla$date)) 
infla$day <- day(ymd(infla$date))
```
There were some words that seemed to be interesting to examine. New York Times Articles were examined but the package gtrendsR allows users to find the number of hits for a keyword. The words "economy", "unemployment", and "inflation" were used as the keyword in the search query. The time frame for the graphs are from January 1, 2007 to December 11, 2022. 

### Economy -- gtrendsR
The number of hits regarding economy seems to hit 100, the highest searched, in 2008 to 2009. The number of hits since then has been steadily increasing and decreasing. 
```{r echo=FALSE}
ggplot(econ_08, aes(x=date, y=hits, group = 1)) +
  geom_line()+
  ggtitle("Number of Hits for Economy")+
  xlab("Year")+
  ylab("Hits")+
  theme(plot.title = element_text(hjust = 0.5))
```
### Unemployment
The number of hits regarding employment hit 100 in 2020.Prior to 2020, the past years did not see hits beyond 20. 
```{r echo=FALSE}
ggplot(unemp, aes(x=date, y=hits, group = 1)) +
  geom_line()+
  ggtitle("Number of Hits for Unemployment")+
  xlab("Year")+
  ylab("Hits")+
  theme(plot.title = element_text(hjust = 0.5))
```
### Inflation
The number of hits regarding inflation steadily increased after 2020. 

```{r echo=FALSE}
ggplot(infla, aes(x=date, y=hits, group = 1)) +
  geom_line()+
  ggtitle("Number of Hits for Inflation")+
  xlab("Year")+
  ylab("Hits")+
  theme(plot.title = element_text(hjust = 0.5))
```

## Sentiment Analysis
A sentiment analysis was done using the New York Times headers and snippets regarding unemployment. This was done to analyze the emotions behind the text to show what the sentiment was for every other year between 2008 to 2022. The website ____ code was used to aid with the sentiment analysis.

### 2008 

The sentiment analysis for 2008 words has the highest percentage score for trust. However, fear and sadness is also at a high. 
```{r echo=FALSE}
senti_08<-dtm_d_2008 %>%
  dplyr::mutate(words = n()) %>%
  dplyr::left_join(tidytext::get_sentiments("nrc")) %>%
  dplyr::mutate(word = factor(word),
         sentiment = factor(sentiment))

senti_08<-senti_08 %>%
  dplyr::group_by(sentiment) %>%
  dplyr::summarise(sentiment = unique(sentiment),
                   sentiment_freq = n(),
                   words = unique(words)) %>%
  dplyr::filter(is.na(sentiment) == F) %>%
  dplyr::mutate(percentage = round(sentiment_freq/words*100, 1))
  
plot_sent_2008<- senti_08 %>%
  dplyr::filter(sentiment != "positive",
         sentiment != "negative") %>%
  ggplot(aes(sentiment, percentage,fill=sentiment)) +    
  geom_bar(stat="identity",   
           position=position_dodge()) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Sentiment Analysis of 2008 NYT Articles")+
  xlab("Sentiment")+
  ylab("Percentage")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 7) 

plot_sent_2008
```

### 2010 
Unlike 2008, the percetange in joy is higher in 2010. Trust has increased and is at 6%. Disgust, anger, and sadness is lower in 2010 as well. 

```{r echo=FALSE}
senti_10<-dtm_d_2010 %>%
  dplyr::mutate(words = n()) %>%
  dplyr::left_join(tidytext::get_sentiments("nrc")) %>%
  dplyr::mutate(word = factor(word),
         sentiment = factor(sentiment))

senti_10<-senti_10 %>%
  dplyr::group_by(sentiment) %>%
  dplyr::summarise(sentiment = unique(sentiment),
                   sentiment_freq = n(),
                   words = unique(words)) %>%
  dplyr::filter(is.na(sentiment) == F) %>%
  dplyr::mutate(percentage = round(sentiment_freq/words*100, 1))
  
plot_sent_2010<- senti_10 %>%
  dplyr::filter(sentiment != "positive",
         sentiment != "negative") %>%
  ggplot(aes(sentiment, percentage,fill=sentiment)) +    
  geom_bar(stat="identity",   
           position=position_dodge()) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Sentiment Analysis of 2010 NYT Articles")+
  xlab("Sentiment")+
  ylab("Percentage")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 7) 

plot_sent_2010
```
### 2012
Trust in 2012 has increased percentage is higher than in previous years. However, anger has also increased. 
```{r echo=FALSE}
senti_12<-dtm_d_2012 %>%
  dplyr::mutate(words = n()) %>%
  dplyr::left_join(tidytext::get_sentiments("nrc")) %>%
  dplyr::mutate(word = factor(word),
         sentiment = factor(sentiment))

senti_12<-senti_12 %>%
  dplyr::group_by(sentiment) %>%
  dplyr::summarise(sentiment = unique(sentiment),
                   sentiment_freq = n(),
                   words = unique(words)) %>%
  dplyr::filter(is.na(sentiment) == F) %>%
  dplyr::mutate(percentage = round(sentiment_freq/words*100, 1))
  
plot_sent_2012<- senti_12 %>%
  dplyr::filter(sentiment != "positive",
         sentiment != "negative") %>%
  ggplot(aes(sentiment, percentage,fill=sentiment)) +    
  geom_bar(stat="identity",   
           position=position_dodge()) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Sentiment Analysis of 2012 NYT Articles")+
  xlab("Sentiment")+
  ylab("Percentage")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0,7)

plot_sent_2012
```
### 2014
Anger and fear has decreased since 2012 where as anticipation and joy has increased.
```{r echo=FALSE}
senti_14<-dtm_d_2014 %>%
  dplyr::mutate(words = n()) %>%
  dplyr::left_join(tidytext::get_sentiments("nrc")) %>%
  dplyr::mutate(word = factor(word),
         sentiment = factor(sentiment))

senti_14<-senti_14 %>%
  dplyr::group_by(sentiment) %>%
  dplyr::summarise(sentiment = unique(sentiment),
                   sentiment_freq = n(),
                   words = unique(words)) %>%
  dplyr::filter(is.na(sentiment) == F) %>%
  dplyr::mutate(percentage = round(sentiment_freq/words*100, 1))
  
plot_sent_2014<- senti_14 %>%
  dplyr::filter(sentiment != "positive",
         sentiment != "negative") %>%
  ggplot(aes(sentiment, percentage,fill=sentiment)) +    
  geom_bar(stat="identity",   
           position=position_dodge()) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Sentiment Analysis of 2014 NYT Articles")+
  xlab("Sentiment")+
  ylab("Percentage")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 7)

plot_sent_2014
```
### 2016
Anger, fear and sadness has seen an increase since 2014.
```{r echo=FALSE}
senti_16<-dtm_d_2016 %>%
  dplyr::mutate(words = n()) %>%
  dplyr::left_join(tidytext::get_sentiments("nrc")) %>%
  dplyr::mutate(word = factor(word),
         sentiment = factor(sentiment))

senti_16<-senti_16 %>%
  dplyr::group_by(sentiment) %>%
  dplyr::summarise(sentiment = unique(sentiment),
                   sentiment_freq = n(),
                   words = unique(words)) %>%
  dplyr::filter(is.na(sentiment) == F) %>%
  dplyr::mutate(percentage = round(sentiment_freq/words*100, 1))
  
plot_sent_2016<- senti_16 %>%
  dplyr::filter(sentiment != "positive",
         sentiment != "negative") %>%
  ggplot(aes(sentiment, percentage,fill=sentiment)) +    
  geom_bar(stat="identity",   
           position=position_dodge()) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Sentiment Analysis of 2016 NYT Articles")+
  xlab("Sentiment")+
  ylab("Percentage")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 7) 

plot_sent_2016
```

```{r echo=FALSE}
senti_18<-dtm_d_2018 %>%
  dplyr::mutate(words = n()) %>%
  dplyr::left_join(tidytext::get_sentiments("nrc")) %>%
  dplyr::mutate(word = factor(word),
         sentiment = factor(sentiment))

senti_18<-senti_18 %>%
  dplyr::group_by(sentiment) %>%
  dplyr::summarise(sentiment = unique(sentiment),
                   sentiment_freq = n(),
                   words = unique(words)) %>%
  dplyr::filter(is.na(sentiment) == F) %>%
  dplyr::mutate(percentage = round(sentiment_freq/words*100, 1))
  
plot_sent_2018<- senti_18 %>%
  dplyr::filter(sentiment != "positive",
         sentiment != "negative") %>%
  ggplot(aes(sentiment, percentage,fill=sentiment)) +    
  geom_bar(stat="identity",   
           position=position_dodge()) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Sentiment Analysis of 2018 NYT Articles")+
  xlab("Sentiment")+
  ylab("Percentage")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 7) 

plot_sent_2018
```

```{r echo=FALSE}
senti_20<-dtm_d_2020 %>%
  dplyr::mutate(words = n()) %>%
  dplyr::left_join(tidytext::get_sentiments("nrc")) %>%
  dplyr::mutate(word = factor(word),
         sentiment = factor(sentiment))

senti_20<-senti_20 %>%
  dplyr::group_by(sentiment) %>%
  dplyr::summarise(sentiment = unique(sentiment),
                   sentiment_freq = n(),
                   words = unique(words)) %>%
  dplyr::filter(is.na(sentiment) == F) %>%
  dplyr::mutate(percentage = round(sentiment_freq/words*100, 1))
  
plot_sent_2020<- senti_20 %>%
  dplyr::filter(sentiment != "positive",
         sentiment != "negative") %>%
  ggplot(aes(sentiment, percentage,fill=sentiment)) +    
  geom_bar(stat="identity",   
           position=position_dodge()) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Sentiment Analysis of 2020 NYT Articles")+
  xlab("Sentiment")+
  ylab("Percentage")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 7)

plot_sent_2020
```

```{r echo=FALSE}
senti_22<-dtm_d_2022 %>%
  dplyr::mutate(words = n()) %>%
  dplyr::left_join(tidytext::get_sentiments("nrc")) %>%
  dplyr::mutate(word = factor(word),
         sentiment = factor(sentiment))

senti_22<-senti_22 %>%
  dplyr::group_by(sentiment) %>%
  dplyr::summarise(sentiment = unique(sentiment),
                   sentiment_freq = n(),
                   words = unique(words)) %>%
  dplyr::filter(is.na(sentiment) == F) %>%
  dplyr::mutate(percentage = round(sentiment_freq/words*100, 1))
  
plot_sent_2022<- senti_22 %>%
  dplyr::filter(sentiment != "positive",
         sentiment != "negative") %>%
  ggplot(aes(sentiment, percentage,fill=sentiment)) +    
  geom_bar(stat="identity",   
           position=position_dodge()) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Sentiment Analysis of 2022 NYT Articles")+
  xlab("Sentiment")+
  ylab("Percentage")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 7) 

plot_sent_2022
```


```{r fig.width=20, fig.height=10}
ggarrange(plot_sent_2008,plot_sent_2010,plot_sent_2012,plot_sent_2014,plot_sent_2016,plot_sent_2018,plot_sent_2020,plot_sent_2022)
```


## Discussion

## Key Take Aways Regarding Top Frequent Words
After examining the top frequent words between 2008 and 2022, a few words reappeared in multiple years. Words like economy, rate, and jobless appeared in more than one year whereas there was also words that only appeared in one year. words like "Obama" or "Pandemic" were only top frequently written in that specific year.

The following questions were asked in the beginning of this project:
  1. How has the wording in headlines and snippets changed regarding unemployment from 2008 to 2022?
      * The wording in the headlines and snippets frequently have the same words such as "economy", "rate", and "joblesS"
  2. Can headline and snippet wording regarding unemployment shed light into other concerns that Americans may have?

## Key Take Aways Regarding gTrendsR
gTrends was used find the number of hits for Google Searches for specific keywords

## Key Take Aways Regarding Sentiment Analysis

## Key Take Aways Regarding Whole Project

### Limitations

### What Could Have Been Done Better
The data cleaning aspect could have been done better. Many of the words repeatedly appeared in the top frequent word portion. These word duplicates made the plots confusing as the same words appeared more than once. 

### How Could This Project Be Expanded
This project could be expanded by examining real unemployment, unemployment benefit, and inflation data.

## References




