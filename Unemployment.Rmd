---
title: "Untitled"
output: html_document
date: "2022-12-11"
bibliography: citations.bib

---

```{r, include = FALSE}
library(knitr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(tm)
library(SnowballC)
library(syuzhet)
library(gtrendsR)
remotes::install_github("news-r/nytimes")
library(jsonlite)
library(stringr)
library(tidytext)
library(textdata)
library(ggpubr)
```

## Introduction
  This project examines headlines and snippets regarding unemployment. Articles from The New York Times are collected from The New York Times API, starting from 2008, the middle of a recession, to the present day. The headlines and snippets are analyzed to find the most frequented words and how the text has changed throughout the years. The keywords in the articles may also highlight other concerns that Americans have that are not related to unemployment, such as inflation. Therefore, this project will answer the following question:
  1. How has the wording in headlines and snippets changed or stayed the same regarding unemployment from 2008 to 2022?

  
### Data Collection
The New York Times API has many unique features. One of the features is the ability to find articles using a specific keyword and year. For this project, I queried the word "unemployment" from every other year between 2008 to 2022. After querying, the result will be a snippet and headline character variable for each year specified. An RPubs tutorial released by Vicentwx aided in collecting the data as they have written an extensive article outlining how to use The New York Times article search feature and how to clean the data [@vincentwx_2019].
```{r include = FALSE}
nytime = function (keyword,year) {
  searchQ = URLencode(keyword)
  url = paste('http://api.nytimes.com/svc/search/v2/articlesearch.json?q=',searchQ,
              '&begin_date=',year,'0101&end_date=',year,'1231&api-key=',api,sep="")
 
  initialsearch = fromJSON(url,flatten = T)
  maxPages = round((initialsearch$response$meta$hits / 10)-1)
  
 
  maxPages = ifelse(maxPages >= 10, 10, maxPages)
  
 
  df = data.frame(id=as.numeric(),created_time=character(),snippet=character(),
                  headline=character())
  
 
  for(i in 0:maxPages){
  
    nytSearch = fromJSON(paste0(url, "&page=", i), flatten = T) 
    temp = data.frame(id=1:nrow(nytSearch$response$docs),
                      created_time = nytSearch$response$docs$pub_date,
                      snippet = nytSearch$response$docs$snippet,
                      headline = nytSearch$response$docs$headline.main)
    df=rbind(df,temp)
    Sys.sleep(5) 
  }
  return(df)
}
```

```{r, include = FALSE}
# These are all in seperate code chunks that the API does not get overwhelmed.
api = "ZMG66AOpWqDJ5OqyK9GSZh4mSrWAyapR"
NY2008 <- nytime('unemployment',2008)
```
```{r, include = FALSE}
NY2010 <- nytime('unemployment',2010)
```

```{r, include = FALSE}
NY2012 <- nytime('unemployment',2012)
```

```{r, include = FALSE}
NY2014 <- nytime('unemployment',2014)
```

```{r, include = FALSE}
NY2016 <- nytime('unemployment',2016)
```

```{r, include = FALSE}
NY2018 <- nytime('unemployment',2018)
```

```{r, include = FALSE}
NY2020 <- nytime('unemployment',2020)
```

```{r, include = FALSE}
NY2022 <- nytime('unemployment',2022)
```

```{r include = FALSE}
write.csv(NY2022, "C:\\Users\\tutu\\Downloads\\NY2022.csv")
write.csv(NY2020, "C:\\Users\\tutu\\Downloads\\NY2020.csv")
write.csv(NY2018, "C:\\Users\\tutu\\Downloads\\NY2018.csv")
write.csv(NY2016, "C:\\Users\\tutu\\Downloads\\NY2016.csv")
write.csv(NY2014, "C:\\Users\\tutu\\Downloads\\NY2014.csv")
write.csv(NY2012, "C:\\Users\\tutu\\Downloads\\NY2012.csv")
write.csv(NY2010, "C:\\Users\\tutu\\Downloads\\NY2010.csv")
write.csv(NY2008, "C:\\Users\\tutu\\Downloads\\NY2008.csv")
```

### Cleaning and Preparing the Data
As mentioned in the prior section, Vicentwx's extensive tutorial on The New York Times article search feature was used to aid in the cleaning and preparation of the data. Each year was made into a corpus which was then removed of all punctuation, white space, and numbers. Afterward, the corpus was made into a term document matrix. This process was repeated for each year from 2008 to 2022. [@vincentwx_2019]

```{r, include = FALSE}
# Adding the appropriate year to the data
U2008<-data.frame(append(NY2008, c(Year='2008'), after=1))

U2010<-data.frame(append(NY2010, c(Year='2010'), after=1))

U2012<-data.frame(append(NY2012, c(Year='2012'), after=1))

U2014<-data.frame(append(NY2014, c(Year='2014'), after=1))

U2016<-data.frame(append(NY2016, c(Year='2016'), after=1))

U2018<-data.frame(append(NY2018, c(Year='2018'), after=1))

U2020<-data.frame(append(NY2020,c(Year='2020'), after=1))

U2022<-data.frame(append(NY2022, c(Year='2022'), after=1))
```


```{r, include = FALSE}
TextDoc2008 <- Corpus(VectorSource(U2008))

edit_text <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

TextDoc2008 <- tm_map(TextDoc2008, edit_text, "/")
TextDoc2008 <- tm_map(TextDoc2008, edit_text, "@")
TextDoc2008 <- tm_map(TextDoc2008, edit_text, "\\|")
TextDoc2008 <- tm_map(TextDoc2008, content_transformer(tolower))
TextDoc2008 <- tm_map(TextDoc2008, removeNumbers)
TextDoc2008 <- tm_map(TextDoc2008, removeWords, stopwords("english"))
TextDoc2008 <- tm_map(TextDoc2008, removePunctuation)
TextDoc2008 <- tm_map(TextDoc2008, stripWhitespace)
TextDoc2010 <- tm_map(TextDoc2008, stemDocument)

head(TextDoc2008)
TextDoc_dtm_2008 <- TermDocumentMatrix(TextDoc2008)

TextDoc_dtm_2008 <- TermDocumentMatrix(TextDoc2008)
dtm_m_2008 <- as.matrix(TextDoc_dtm_2008)
```


```{r, include = FALSE}
dtm_v_2008 <- sort(rowSums(dtm_m_2008),decreasing=TRUE)
dtm_d_2008 <- data.frame(word = names(dtm_v_2008),freq=dtm_v_2008)

```
After the cleaning and the preparation, the data is ready to be used for analysis. Below is a table of the 2008 top frequented words in the article sample that was collected. 
```{r}
library(gt)
head(dtm_d_2008, 5)

dtm_d_2008%>%
  head(5)%>%
  gt()%>%
  tab_header(title="2008 Top Words")
```

```{r, include = FALSE}
TextDoc2010 <- Corpus(VectorSource(U2010))

edit_text <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

TextDoc2010 <- tm_map(TextDoc2010, edit_text, "/")
TextDoc2010 <- tm_map(TextDoc2010, edit_text, "@")
TextDoc2010 <- tm_map(TextDoc2010, edit_text, "\\|")
TextDoc2010 <- tm_map(TextDoc2010, content_transformer(tolower))
TextDoc2010 <- tm_map(TextDoc2010, removeNumbers)
TextDoc2010 <- tm_map(TextDoc2010, removeWords, stopwords("english"))
TextDoc2010 <- tm_map(TextDoc2010, removePunctuation)
TextDoc2010 <- tm_map(TextDoc2010, stripWhitespace)
TextDoc2010 <- tm_map(TextDoc2010, stemDocument)

head(TextDoc2010)
TextDoc_dtm_2010 <- TermDocumentMatrix(TextDoc2010)

TextDoc_dtm_2010 <- TermDocumentMatrix(TextDoc2010)
dtm_m_2010 <- as.matrix(TextDoc_dtm_2010)

# Sort by descearing value of frequency
dtm_v_2010 <- sort(rowSums(dtm_m_2010),decreasing=TRUE)
dtm_d_2010 <- data.frame(word = names(dtm_v_2010),freq=dtm_v_2010)
# Display the top 5 most frequent words
head(dtm_d_2010, 5)
```

```{r, include = FALSE}
TextDoc2012 <- Corpus(VectorSource(U2012))

edit_text <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

TextDoc2012 <- tm_map(TextDoc2012, edit_text, "/")
TextDoc2012 <- tm_map(TextDoc2012, edit_text, "@")
TextDoc2012 <- tm_map(TextDoc2012, edit_text, "\\|")
TextDoc2012 <- tm_map(TextDoc2012, content_transformer(tolower))
TextDoc2012 <- tm_map(TextDoc2012, removeNumbers)
TextDoc2012 <- tm_map(TextDoc2012, removeWords, stopwords("english"))
TextDoc2012 <- tm_map(TextDoc2012, removePunctuation)
TextDoc2012 <- tm_map(TextDoc2012, stripWhitespace)
TextDoc2012 <- tm_map(TextDoc2012, stemDocument)

head(TextDoc2012)
TextDoc_dtm_2012 <- TermDocumentMatrix(TextDoc2012)

TextDoc_dtm_2012 <- TermDocumentMatrix(TextDoc2012)
dtm_m_2012 <- as.matrix(TextDoc_dtm_2012)

# Sort by descearing value of frequency
dtm_v_2012 <- sort(rowSums(dtm_m_2012),decreasing=TRUE)
dtm_d_2012 <- data.frame(word = names(dtm_v_2012),freq=dtm_v_2012)
# Display the top 5 most frequent words
head(dtm_d_2012, 5)
```

```{r, include = FALSE}
TextDoc2014 <- Corpus(VectorSource(U2014))

edit_text <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

TextDoc2014 <- tm_map(TextDoc2014, edit_text, "/")
TextDoc2014 <- tm_map(TextDoc2014, edit_text, "@")
TextDoc2014 <- tm_map(TextDoc2014, edit_text, "\\|")
TextDoc2014 <- tm_map(TextDoc2014, content_transformer(tolower))
TextDoc2014 <- tm_map(TextDoc2014, removeNumbers)
TextDoc2014 <- tm_map(TextDoc2014, removeWords, stopwords("english"))
TextDoc2014 <- tm_map(TextDoc2014, removePunctuation)
TextDoc2014 <- tm_map(TextDoc2014, stripWhitespace)
TextDoc2014 <- tm_map(TextDoc2014, stemDocument)

head(TextDoc2014)
TextDoc_dtm_2014 <- TermDocumentMatrix(TextDoc2014)

TextDoc_dtm_2014 <- TermDocumentMatrix(TextDoc2014)
dtm_m_2014 <- as.matrix(TextDoc_dtm_2014)

# Sort by descearing value of frequency
dtm_v_2014 <- sort(rowSums(dtm_m_2014),decreasing=TRUE)
dtm_d_2014 <- data.frame(word = names(dtm_v_2014),freq=dtm_v_2014)
# Display the top 5 most frequent words
head(dtm_d_2014, 5)
```

```{r, include = FALSE}
TextDoc2016 <- Corpus(VectorSource(U2016))

edit_text <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

TextDoc2016 <- tm_map(TextDoc2016, edit_text, "/")
TextDoc2016 <- tm_map(TextDoc2016, edit_text, "@")
TextDoc2016 <- tm_map(TextDoc2016, edit_text, "\\|")
TextDoc2016 <- tm_map(TextDoc2016, content_transformer(tolower))
TextDoc2016 <- tm_map(TextDoc2016, removeNumbers)
TextDoc2016 <- tm_map(TextDoc2016, removeWords, stopwords("english"))
TextDoc2016 <- tm_map(TextDoc2016, removePunctuation)
TextDoc2016 <- tm_map(TextDoc2016, stripWhitespace)
TextDoc2016 <- tm_map(TextDoc2016, stemDocument)

head(TextDoc2016)
TextDoc_dtm_2016 <- TermDocumentMatrix(TextDoc2016)

TextDoc_dtm_2016 <- TermDocumentMatrix(TextDoc2016)
dtm_m_2016 <- as.matrix(TextDoc_dtm_2016)

# Sort by descearing value of frequency
dtm_v_2016 <- sort(rowSums(dtm_m_2016),decreasing=TRUE)
dtm_d_2016 <- data.frame(word = names(dtm_v_2016),freq=dtm_v_2016)
# Display the top 5 most frequent words
head(dtm_d_2016, 5)
```


```{r, include = FALSE}
TextDoc2018 <- Corpus(VectorSource(U2018))

edit_text <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

TextDoc2018 <- tm_map(TextDoc2018, edit_text, "/")
TextDoc2018 <- tm_map(TextDoc2018, edit_text, "@")
TextDoc2018 <- tm_map(TextDoc2018, edit_text, "\\|")
TextDoc2018 <- tm_map(TextDoc2018, content_transformer(tolower))
TextDoc2018 <- tm_map(TextDoc2018, removeNumbers)
TextDoc2018 <- tm_map(TextDoc2018, removeWords, stopwords("english"))
TextDoc2018 <- tm_map(TextDoc2018, removePunctuation)
TextDoc2018 <- tm_map(TextDoc2018, stripWhitespace)
TextDoc2018 <- tm_map(TextDoc2018, stemDocument)

head(TextDoc2018)
TextDoc_dtm_2018 <- TermDocumentMatrix(TextDoc2018)

TextDoc_dtm_2018 <- TermDocumentMatrix(TextDoc2018)
dtm_m_2018 <- as.matrix(TextDoc_dtm_2018)

# Sort by descearing value of frequency
dtm_v_2018 <- sort(rowSums(dtm_m_2018),decreasing=TRUE)
dtm_d_2018 <- data.frame(word = names(dtm_v_2018),freq=dtm_v_2018)
# Display the top 5 most frequent words
head(dtm_d_2018, 5)
```

```{r, include = FALSE}
TextDoc2020 <- Corpus(VectorSource(U2020))

edit_text <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

TextDoc2020 <- tm_map(TextDoc2020, edit_text, "/")
TextDoc2020 <- tm_map(TextDoc2020, edit_text, "@")
TextDoc2020 <- tm_map(TextDoc2020, edit_text, "\\|")
TextDoc2020 <- tm_map(TextDoc2020, content_transformer(tolower))
TextDoc2020 <- tm_map(TextDoc2020, removeNumbers)
TextDoc2020 <- tm_map(TextDoc2020, removeWords, stopwords("english"))
TextDoc2020 <- tm_map(TextDoc2020, removePunctuation)
TextDoc2020 <- tm_map(TextDoc2020, stripWhitespace)
TextDoc2020 <- tm_map(TextDoc2020, stemDocument)

head(TextDoc2020)
TextDoc_dtm_2020 <- TermDocumentMatrix(TextDoc2020)

TextDoc_dtm_2020 <- TermDocumentMatrix(TextDoc2020)
dtm_m_2020 <- as.matrix(TextDoc_dtm_2020)

# Sort by descearing value of frequency
dtm_v_2020 <- sort(rowSums(dtm_m_2020),decreasing=TRUE)
dtm_d_2020 <- data.frame(word = names(dtm_v_2020),freq=dtm_v_2020)
# Display the top 5 most frequent words
head(dtm_d_2020, 5)
```

```{r, include = FALSE}
TextDoc2022 <- Corpus(VectorSource(U2022))

edit_text <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

TextDoc2022 <- tm_map(TextDoc2022, edit_text, "/")
TextDoc2022 <- tm_map(TextDoc2022, edit_text, "@")
TextDoc2022 <- tm_map(TextDoc2022, edit_text, "\\|")
TextDoc2022 <- tm_map(TextDoc2022, content_transformer(tolower))
TextDoc2022 <- tm_map(TextDoc2022, removeNumbers)
TextDoc2022 <- tm_map(TextDoc2022, removeWords, stopwords("english"))
TextDoc2022 <- tm_map(TextDoc2022, removePunctuation)
TextDoc2022 <- tm_map(TextDoc2022, stripWhitespace)
TextDoc2022 <- tm_map(TextDoc2022, stemDocument)

head(TextDoc2022)
TextDoc_dtm_2022 <- TermDocumentMatrix(TextDoc2022)

TextDoc_dtm_2022 <- TermDocumentMatrix(TextDoc2022)
dtm_m_2022 <- as.matrix(TextDoc_dtm_2022)

# Sort by descearing value of frequency
dtm_v_2022 <- sort(rowSums(dtm_m_2022),decreasing=TRUE)
dtm_d_2022 <- data.frame(word = names(dtm_v_2022),freq=dtm_v_2022)
# Display the top 5 most frequent words

head(dtm_d_2022)
dtm_d_2022
```

### Explanatory Analysis
For the first part of the analysis, "unemployment" must be removed from the sample. It is a common word that appears often in the sample since the word used to query  was "unemployment". 

Barplots will be used to compare the top frequently appearing words in the article sample. Each article has the same Y limit (0,45) to make graph comparison easier in first glance. The word with the highest frequency is also highlighted in blue to aid in identification of the top frequented word. 

```{r, include = FALSE}
head(dtm_d_2008)
dtm_d_2008$word.f <-as.factor(dtm_d_2008$word)
top2008<-dtm_d_2008%>%
  slice(3:8)

plot_08<-ggplot(data= top2008,aes(word.f, freq,fill=freq)) +    
  geom_bar(stat="identity",   
           position=position_dodge(),) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Top Frequented Words in NYT Articles \n Regarding Unemployment in 2008")+
  xlab("Word")+ ylab("Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 45)
```

### 2008

The 2008 plot indicates some of the errors in the data collection and data preparation portion. Some of the words are similar such as economic and economy. Despite not being the same word, "economic"  and "economy" could have been grouped. Additionally, the plot has "new" and "york" as the top words. There is a strong possibility that these two words are top frequenters as the newspaper is The New York Times. In a future analysis, the words "new" and "york" may have to be omitted from the sample. Sadly due to time constraints, I was unable to fix these errors. 

```{r}
plot_08
```

```{r, include = FALSE}
head(dtm_d_2010)
dtm_d_2010$word.f <-as.factor(dtm_d_2010$word)
top2010<-dtm_d_2010%>%
  slice(3:8)

plot_10<-ggplot(data= top2010,aes(word.f, freq,fill=freq)) +    
  geom_bar(stat="identity",   
           position=position_dodge(),) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Top Frequented Words in NYT Articles \n Regarding Unemployment in 2010")+
  xlab("Word")+ ylab("Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 45)

plot_10
```
### 2010
The top frequented words for NYT articles in 2010 are "economi", "jobless", "new", "Obama", "rate", and "recess". The word "Obama" was the most interesting to see as the word was not a top frequenter in 2008. The jobless, rate, new, and economi are still top frequenters.
```{r}
plot_10
```

```{r, include=FALSE}
head(dtm_d_2012)
dtm_d_2012$word.f <-as.factor(dtm_d_2012$word)
top2012<-dtm_d_2012%>%
  slice(3:8)

plot_12<-ggplot(data= top2012,aes(word.f, freq,fill=freq)) +    
  geom_bar(stat="identity",   
           position=position_dodge(),) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Top Frequented Words in NYT Articles \n Regarding Unemployment in 2012")+
  xlab("Word")+ ylab("Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 45)
```

### 2012
Not much is different between 2012 and the previous years. There are less articles using the word "jobless" than before. 
```{r}
plot_12
```


```{r, include = FALSE}
head(dtm_d_2014)
dtm_d_2014$word.f <-as.factor(dtm_d_2014$word)
top2014<-dtm_d_2014%>%
  slice(3:8)

plot_14<-ggplot(data= top2014,aes(word.f, freq,fill=freq)) +    
  geom_bar(stat="identity",   
           position=position_dodge(),) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Top Frequented Words in NYT Articles \n Regarding Unemployment in 2014")+
  xlab("Word")+ ylab("Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 45)
```

### 2014
In 2014, the top word in articles regarding unemployment is the word "rate" and is higher in frequency than in past years.
```{r}
plot_14
```

### 2016
```{r, include = FALSE}
head(dtm_d_2016)
dtm_d_2016$word.f <-as.factor(dtm_d_2016$word)
top2016<-dtm_d_2016%>%
  slice(2,4:8)

plot_16<-ggplot(data= top2016,aes(word.f, freq,fill=freq)) +    
  geom_bar(stat="identity",   
           position=position_dodge(),) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Top Frequented Words in NYT Articles \n Regarding Unemployment in 2016")+
  xlab("Word")+ ylab("Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 45)
```

In 2016, a high frequency of articles includes "economy". A reason for the increase in articles revolving the economy could be due to Former President Trump being elected into office.  
```{r}
plot_16
```

### 2018
```{r, include= FALSE}
head(dtm_d_2018)
dtm_d_2018$word.f <-as.factor(dtm_d_2018$word)
top2018<-dtm_d_2018%>%
  slice(3:8)

plot_18<-ggplot(data= top2018,aes(word.f, freq,fill=freq)) +    
  geom_bar(stat="identity",   
           position=position_dodge(),) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Top Frequented Words in NYT Articles \n Regarding Unemployment in 2018")+
  xlab("Word")+ ylab("Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 45)
```
In 2018, the articles including unemployment and economy is higher than previous years including 2016. Business is also a top word that has not appeared in the top in past years. 
```{r}
plot_18
```
### 2020
```{r, include=FALSE}
head(dtm_d_2020)
dtm_d_2020$word.f <-as.factor(dtm_d_2020$word)
top2020<-dtm_d_2020%>%
  slice(2,4:8)

plot_20<-ggplot(data= top2020,aes(word.f, freq,fill=freq)) +    
  geom_bar(stat="identity",   
           position=position_dodge(),) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Top Frequented Words in NYT Articles \n Regarding Unemployment in 2020")+
  xlab("Word")+ ylab("Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 45)
```
One of the top words for this year is pandemic as COVID-19 led to shut downs and job loss concerns. People began turning to benefits due to unemployment and shut downs that forced many to stay at home. 
```{r}
plot_20
```
### 2022

```{r, include= FALSE}
head(dtm_d_2022)
dtm_d_2022$word.f <-as.factor(dtm_d_2022$word)
top2022<-dtm_d_2022%>%
  slice(1,3:4,6:8)

plot_22<-ggplot(data= top2022,aes(word.f, freq,fill=freq)) +    
  geom_bar(stat="identity",   
           position=position_dodge(),) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Top Frequented Words in NYT Articles \n Regarding Unemployment in 2022")+
  xlab("Word")+ ylab("Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 45)
```
In 2022, economy is still an important word as well as rate. The word "inflation" is a top frequent word that has not appeared in previous years.  
```{r}
plot_22
```

Below is a plot of all the bar charts combined. 
```{r echo=FALSE ,fig.width=20, fig.height=10}
ggarrange(plot_08,plot_10,plot_12,plot_14,plot_16,plot_18,plot_20,plot_22)
```

```{r, include = FALSE}
dtm_d_2008.copy<-data.frame(append(dtm_d_2008, c(Year='2008'), after=1))

dtm_d_2010.copy<-data.frame(append(dtm_d_2010, c(Year='2010'), after=1))

dtm_d_2012.copy<-data.frame(append(dtm_d_2012, c(Year='2012'), after=1))

dtm_d_2014.copy<-data.frame(append(dtm_d_2014, c(Year='2014'), after=1))

dtm_d_2016.copy<-data.frame(append(dtm_d_2016, c(Year='2016'), after=1))

dtm_d_2018.copy<-data.frame(append(dtm_d_2018, c(Year='2018'), after=1))

dtm_d_2020.copy<-data.frame(append(dtm_d_2020,c(Year='2020'), after=1))

dtm_d_2022.copy<-data.frame(append(dtm_d_2022, c(Year='2022'), after=1))

total <-rbind(dtm_d_2008.copy,dtm_d_2010.copy,dtm_d_2012.copy,dtm_d_2014.copy,dtm_d_2016.copy,dtm_d_2018.copy,dtm_d_2020.copy,dtm_d_2022.copy)

total$Year<-factor(total$Year)
```

### Time Series Plot

For the next portion of the analysis, a line plot has been made to indicate the frequency of specific words between 2008 and 2022. A new data frame has been made with all the words and years combined. Each line on the line plot represents a different word. The blue line indicates the frequency of "economi" and "economy". The green line indicates the frequency of the "benefit". The purple line indicates the frequency of "rate". Lastly, the black line indicates the frequency of "unemployment" and "unemploy". Sadly, a legend was unable to be made, which would have made identifying the lines easier. 

The black line for "unemployment" was the highest in the sample in 2008 during the recession. The black line is the lowest in 2022. The purple line for "rate" is the highest for the sample in 2014 and the lowest in 2020. The green line regarding "benefit" began in 2010, perhaps in conjunction with Obama Care. The green line hit a peak in 2020 when the COVID-19 Pandemic led to shut downs. The blue line for "economy" indicates the peak for the sample occurred in 2022. 

```{r echo=FALSE}
word_econ<-filter(total, word.f=="economi"|word.f=="economy")
word_econ$Year<-as.numeric(word_econ$Year)
word_econ


word_benefit<-filter(total, word.f=="benefit")
word_benefit$Year<-as.numeric(word_benefit$Year)
word_benefit

word_rate<-filter(total, word.f=="rate")
word_rate$Year<-as.numeric(word_rate$Year)
word_rate

word_r<-filter(total, word.f=="unemploy"|word.f=="unemployment")
word_r$Year<-as.numeric(word_r$Year)
word_r

word_econ
Complete.ts.plot <- ggplot()+
  geom_line(data=word_econ, aes(x=Year, y=freq), color="blue")+
  geom_line(data=word_benefit, aes(x=Year, y=freq), color='green')+
  geom_line(data=word_rate, aes(x=Year, y=freq), color='purple')+
  geom_line(data=word_r, aes(x=Year, y=freq), color='black')+
  ggtitle("Frequency of NYT Words")+
  xlab("Year")+
  ylab("Freq")+
  theme(plot.title = element_text(hjust = 0.5))+ 
  theme_minimal() +
     scale_color_manual(name = "Model fit",
                        breaks = c("economy", "benefit", "rate","unemployment"),
                        values = c("economy" = "blue", "benefit" = "green", "rate" = "purple", "unemployment"="black") ) # sadly the legend did not work

Complete.ts.plot
```


##Gtrends
```{r echo=FALSE}
econ_08.1 <- gtrends(keyword="economy", time="2007-01-01 2022-12-11",geo="US")

econ_08<-as.data.frame(econ_08.1$interest_over_time)
library(lubridate)
econ_08$year <- year(ymd(econ_08$date))
econ_08$month <- month(ymd(econ_08$date)) 
econ_08$day <- day(ymd(econ_08$date))

econ_08

unemp <- gtrends(keyword="Unemployment", time="2007-01-01 2022-12-11",geo="US")

unemp<-as.data.frame(unemp$interest_over_time)
unemp$year <- year(ymd(unemp$date))
unemp$month <- month(ymd(unemp$date)) 
unemp$day <- day(ymd(unemp$date))

unemp


infla <- gtrends(keyword="inflation", time="2007-01-01 2022-12-11",geo="US")

infla<-as.data.frame(infla$interest_over_time)
infla$year <- year(ymd(infla$date))
infla$month <- month(ymd(infla$date)) 
infla$day <- day(ymd(infla$date))
```
There were some words that seemed to be interesting to examine. New York Times Articles were examined but the package gtrendsR allows users to find the number of hits for a keyword. The words "economy", "unemployment", and "inflation" were used as the keyword in the search query. The time frame for the graphs are from January 1, 2007 to December 11, 2022. 

### Economy -- gtrendsR
The number of hits regarding economy seems to hit 100, the highest searched, in 2008 to 2009. The number of hits since then has been steadily increasing and decreasing. 
```{r echo=FALSE}
ggplot(econ_08, aes(x=date, y=hits, group = 1)) +
  geom_line()+
  ggtitle("Number of Hits for Economy")+
  xlab("Year")+
  ylab("Hits")+
  theme(plot.title = element_text(hjust = 0.5))
```
### Unemployment
The number of hits regarding employment hit 100 in 2020.Prior to 2020, the past years did not see hits beyond 20. 
```{r echo=FALSE}
ggplot(unemp, aes(x=date, y=hits, group = 1)) +
  geom_line()+
  ggtitle("Number of Hits for Unemployment")+
  xlab("Year")+
  ylab("Hits")+
  theme(plot.title = element_text(hjust = 0.5))
```
### Inflation
The number of hits regarding inflation steadily increased after 2020. 

```{r echo=FALSE}
ggplot(infla, aes(x=date, y=hits, group = 1)) +
  geom_line()+
  ggtitle("Number of Hits for Inflation")+
  xlab("Year")+
  ylab("Hits")+
  theme(plot.title = element_text(hjust = 0.5))
```

## Sentiment Analysis
A Sentiment analysis using the New York Times headers and snippets regarding unemployment is the next portion of the analysis. The sentiment analysis analyzes emotions behind the text to show what the sentiment was for every other year between 2008 to 2022. The article "Text Mining and Sentiment Analysis: Analysis with R" was used to aid in the sentiment analysis. [@senti]

### 2008 

The sentiment analysis for 2008 words has the highest percentage score for trust. However, fear and sadness is also at a high. 
```{r echo=FALSE}
senti_08<-dtm_d_2008 %>%
  dplyr::mutate(words = n()) %>%
  dplyr::left_join(tidytext::get_sentiments("nrc")) %>%
  dplyr::mutate(word = factor(word),
         sentiment = factor(sentiment))

senti_08<-senti_08 %>%
  dplyr::group_by(sentiment) %>%
  dplyr::summarise(sentiment = unique(sentiment),
                   sentiment_freq = n(),
                   words = unique(words)) %>%
  dplyr::filter(is.na(sentiment) == F) %>%
  dplyr::mutate(percentage = round(sentiment_freq/words*100, 1))
  
plot_sent_2008<- senti_08 %>%
  dplyr::filter(sentiment != "positive",
         sentiment != "negative") %>%
  ggplot(aes(sentiment, percentage,fill=sentiment)) +    
  geom_bar(stat="identity",   
           position=position_dodge()) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Sentiment Analysis of 2008 NYT Articles")+
  xlab("Sentiment")+
  ylab("Percentage")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 7) 

plot_sent_2008
```

### 2010 
Unlike 2008, the percetange in joy is higher in 2010. Trust has increased and is at 6%. Disgust, anger, and sadness is lower in 2010 as well. 

```{r echo=FALSE}
senti_10<-dtm_d_2010 %>%
  dplyr::mutate(words = n()) %>%
  dplyr::left_join(tidytext::get_sentiments("nrc")) %>%
  dplyr::mutate(word = factor(word),
         sentiment = factor(sentiment))

senti_10<-senti_10 %>%
  dplyr::group_by(sentiment) %>%
  dplyr::summarise(sentiment = unique(sentiment),
                   sentiment_freq = n(),
                   words = unique(words)) %>%
  dplyr::filter(is.na(sentiment) == F) %>%
  dplyr::mutate(percentage = round(sentiment_freq/words*100, 1))
  
plot_sent_2010<- senti_10 %>%
  dplyr::filter(sentiment != "positive",
         sentiment != "negative") %>%
  ggplot(aes(sentiment, percentage,fill=sentiment)) +    
  geom_bar(stat="identity",   
           position=position_dodge()) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Sentiment Analysis of 2010 NYT Articles")+
  xlab("Sentiment")+
  ylab("Percentage")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 7) 

plot_sent_2010
```
### 2012
Trust in 2012 has increased percentage is higher than in previous years. However, anger has also increased. 
```{r echo=FALSE}
senti_12<-dtm_d_2012 %>%
  dplyr::mutate(words = n()) %>%
  dplyr::left_join(tidytext::get_sentiments("nrc")) %>%
  dplyr::mutate(word = factor(word),
         sentiment = factor(sentiment))

senti_12<-senti_12 %>%
  dplyr::group_by(sentiment) %>%
  dplyr::summarise(sentiment = unique(sentiment),
                   sentiment_freq = n(),
                   words = unique(words)) %>%
  dplyr::filter(is.na(sentiment) == F) %>%
  dplyr::mutate(percentage = round(sentiment_freq/words*100, 1))
  
plot_sent_2012<- senti_12 %>%
  dplyr::filter(sentiment != "positive",
         sentiment != "negative") %>%
  ggplot(aes(sentiment, percentage,fill=sentiment)) +    
  geom_bar(stat="identity",   
           position=position_dodge()) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Sentiment Analysis of 2012 NYT Articles")+
  xlab("Sentiment")+
  ylab("Percentage")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0,7)

plot_sent_2012
```
### 2014
Anger and fear has decreased since 2012 where as anticipation and joy has increased.
```{r echo=FALSE}
senti_14<-dtm_d_2014 %>%
  dplyr::mutate(words = n()) %>%
  dplyr::left_join(tidytext::get_sentiments("nrc")) %>%
  dplyr::mutate(word = factor(word),
         sentiment = factor(sentiment))

senti_14<-senti_14 %>%
  dplyr::group_by(sentiment) %>%
  dplyr::summarise(sentiment = unique(sentiment),
                   sentiment_freq = n(),
                   words = unique(words)) %>%
  dplyr::filter(is.na(sentiment) == F) %>%
  dplyr::mutate(percentage = round(sentiment_freq/words*100, 1))
  
plot_sent_2014<- senti_14 %>%
  dplyr::filter(sentiment != "positive",
         sentiment != "negative") %>%
  ggplot(aes(sentiment, percentage,fill=sentiment)) +    
  geom_bar(stat="identity",   
           position=position_dodge()) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Sentiment Analysis of 2014 NYT Articles")+
  xlab("Sentiment")+
  ylab("Percentage")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 7)

plot_sent_2014
```
### 2016
Anger, fear and sadness has seen an increase since 2014.
```{r echo=FALSE}
senti_16<-dtm_d_2016 %>%
  dplyr::mutate(words = n()) %>%
  dplyr::left_join(tidytext::get_sentiments("nrc")) %>%
  dplyr::mutate(word = factor(word),
         sentiment = factor(sentiment))

senti_16<-senti_16 %>%
  dplyr::group_by(sentiment) %>%
  dplyr::summarise(sentiment = unique(sentiment),
                   sentiment_freq = n(),
                   words = unique(words)) %>%
  dplyr::filter(is.na(sentiment) == F) %>%
  dplyr::mutate(percentage = round(sentiment_freq/words*100, 1))
  
plot_sent_2016<- senti_16 %>%
  dplyr::filter(sentiment != "positive",
         sentiment != "negative") %>%
  ggplot(aes(sentiment, percentage,fill=sentiment)) +    
  geom_bar(stat="identity",   
           position=position_dodge()) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Sentiment Analysis of 2016 NYT Articles")+
  xlab("Sentiment")+
  ylab("Percentage")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 7) 

plot_sent_2016
```
### 2018
Anticipation, fear, and anger has decreased while trust remains high.
```{r echo=FALSE}
senti_18<-dtm_d_2018 %>%
  dplyr::mutate(words = n()) %>%
  dplyr::left_join(tidytext::get_sentiments("nrc")) %>%
  dplyr::mutate(word = factor(word),
         sentiment = factor(sentiment))

senti_18<-senti_18 %>%
  dplyr::group_by(sentiment) %>%
  dplyr::summarise(sentiment = unique(sentiment),
                   sentiment_freq = n(),
                   words = unique(words)) %>%
  dplyr::filter(is.na(sentiment) == F) %>%
  dplyr::mutate(percentage = round(sentiment_freq/words*100, 1))
  
plot_sent_2018<- senti_18 %>%
  dplyr::filter(sentiment != "positive",
         sentiment != "negative") %>%
  ggplot(aes(sentiment, percentage,fill=sentiment)) +    
  geom_bar(stat="identity",   
           position=position_dodge()) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Sentiment Analysis of 2018 NYT Articles")+
  xlab("Sentiment")+
  ylab("Percentage")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 7) 

plot_sent_2018
```
### 2020

Anticipation has increased again from 2018, as well as fear. Sadness has also increased. These increases may be caused by the COVID-19 Pandemic. 
```{r echo=FALSE}
senti_20<-dtm_d_2020 %>%
  dplyr::mutate(words = n()) %>%
  dplyr::left_join(tidytext::get_sentiments("nrc")) %>%
  dplyr::mutate(word = factor(word),
         sentiment = factor(sentiment))

senti_20<-senti_20 %>%
  dplyr::group_by(sentiment) %>%
  dplyr::summarise(sentiment = unique(sentiment),
                   sentiment_freq = n(),
                   words = unique(words)) %>%
  dplyr::filter(is.na(sentiment) == F) %>%
  dplyr::mutate(percentage = round(sentiment_freq/words*100, 1))
  
plot_sent_2020<- senti_20 %>%
  dplyr::filter(sentiment != "positive",
         sentiment != "negative") %>%
  ggplot(aes(sentiment, percentage,fill=sentiment)) +    
  geom_bar(stat="identity",   
           position=position_dodge()) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Sentiment Analysis of 2020 NYT Articles")+
  xlab("Sentiment")+
  ylab("Percentage")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 7)

plot_sent_2020
```
### 2022
Anticipation is at a high at heights comparable to trust.
```{r echo=FALSE}
senti_22<-dtm_d_2022 %>%
  dplyr::mutate(words = n()) %>%
  dplyr::left_join(tidytext::get_sentiments("nrc")) %>%
  dplyr::mutate(word = factor(word),
         sentiment = factor(sentiment))

senti_22<-senti_22 %>%
  dplyr::group_by(sentiment) %>%
  dplyr::summarise(sentiment = unique(sentiment),
                   sentiment_freq = n(),
                   words = unique(words)) %>%
  dplyr::filter(is.na(sentiment) == F) %>%
  dplyr::mutate(percentage = round(sentiment_freq/words*100, 1))
  
plot_sent_2022<- senti_22 %>%
  dplyr::filter(sentiment != "positive",
         sentiment != "negative") %>%
  ggplot(aes(sentiment, percentage,fill=sentiment)) +    
  geom_bar(stat="identity",   
           position=position_dodge()) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Sentiment Analysis of 2022 NYT Articles")+
  xlab("Sentiment")+
  ylab("Percentage")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(0, 7) 

plot_sent_2022
```


```{r fig.width=20, fig.height=10}
ggarrange(plot_sent_2008,plot_sent_2010,plot_sent_2012,plot_sent_2014,plot_sent_2016,plot_sent_2018,plot_sent_2020,plot_sent_2022)
```


## Discussion

### Key Take Aways Regarding Top Frequent Words
After examining the top frequent words between 2008 and 2022, a few words reappeared in multiple years. Words like "economy", "rate", and "jobless" appeared in more than one year, whereas there was also words that only appeared in one year. Words like "Obama" or "Pandemic" were only the top frequently words written in specific years, which are unique to them. 

### Key Take Aways Regarding gTrendsR
gTrendsR was used to find the number of hits for Google Searches for specific keywords. The number of hits was specified to the geographical location of the United States. The Google Trends API indicates that the searches for "Unemployment" and "Inflation" are much more searched after 2020 than before it. Whereas, searches for the word "economy" were the highest in 2008.

### Key Take Aways Regarding Sentiment Analysis
The sentiment analysis for this project's sample of articles indicates that the trust sentiment is higher than other sentiments for all years except 2022. In 2022, the anticipation sentiment is higher than in past years. In general, all the sentiments changed levels throughout the years. There were increases in the joy sentiment, which may indicate a decrease in unemployment. If I were to expand on this project, comparing the increases in joy with the unemployment rate would be interesting!

### Key Take Aways Regarding Whole Project
The following question was asked in the beginning of this project:
 1. How has the wording in headlines and snippets changed or stayed the same regarding unemployment from 2008 to 2022?

While it is not possible to make a general statement, from this specific sample, the data indicate that similar words were used from 2008 to 2022. Words such as the economy, rate, and jobless were all commonly used in headers and snippets throughout the years. However, in specific years, there may be more articles released regarding special topics concerning unemployment such as a new president, a heavier emphasis on the economy, or inflation is high. 
      
### Limitations
The New York Times API makes it difficult for R users to collect a large amount of data. Due to this, the sample size of articles was lower than I initially wanted to use. 

### What Could Have Been Done Better
The data cleaning aspect could have been done better. Many of the words repeatedly appeared in the top frequent word portion. Word duplicates make the plots confusing since the same words appears more than once. Additionally, I also, could have found a more efficient way to gather data. 

### How Could This Project Be Expanded
This project could be expanded by examining real unemployment, unemployment benefit, and inflation data. This project could also be expanded by using a larger sample of articles and being more specific in queries. 

## References




